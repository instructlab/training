# SPDX-License-Identifier: Apache-2.0

# These are synchronized with instructlab repo; we have to keep them in sync at
# least until we no longer tie training repo CI with ilab repo through e2e jobs.
torch<2.7.0
vllm<0.9.0

# flash-attn 2.8.0+ is broken for torch 2.6.x.
# See: https://github.com/Dao-AILab/flash-attention/issues/1717
flash-attn<2.8.0
