{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oleg/miniforge3/envs/arbitrary-chat-templates/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "granite = AutoTokenizer.from_pretrained('ibm-granite/granite-3.1-8b-instruct')\n",
    "llama = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B-Instruct')\n",
    "phi = AutoTokenizer.from_pretrained('microsoft/phi-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189, 246, 227, 131]\n",
      "potentially found a glyph id match starting at i=3\n",
      "potentially found a glyph id match starting at i=12\n",
      "potentially found a glyph id match starting at i=21\n",
      "after extending, j=7 is set to placeholder_ids[j]=0\n",
      "detected eos token id, proceeding forward\n",
      "after extending, j=16 is set to placeholder_ids[j]=0\n",
      "detected eos token id, proceeding forward\n",
      "after extending, j=25 is set to placeholder_ids[j]=0\n",
      "detected eos token id, proceeding forward\n",
      "[49152, 2946, 49153, 8279, 5788, 19, 0, 203, 49152, 496, 49153, 42808, 107, 787, 283, 0, 203, 49152, 17594, 49153, 8279, 645, 322, 1604, 5209, 0, 203]\n",
      "[-100, -100, -100, 8279, 5788, 19, 0, -100, -100, -100, -100, 42808, 107, 787, 283, 0, -100, -100, -100, -100, 8279, 645, 322, 1604, 5209, 0, -100]\n",
      "<|start_of_role|>system<|end_of_role|>Hello world!<|end_of_text|>\n",
      "<|start_of_role|>user<|end_of_role|>Holey shit<|end_of_text|>\n",
      "<|start_of_role|>assistant<|end_of_role|>Hello from the other side<|end_of_text|>\n",
      "\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import typing as t\n",
    "# we want to unmask everything BUT the system\n",
    "msgs = [\n",
    "    {\n",
    "        \"content\": \"Hello world!\",\n",
    "        \"role\": \"system\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Holey shit\",\n",
    "        \"role\": \"user\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Hello from the other side\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "]\n",
    "\n",
    "GLYPH = \"ð“€´\"\n",
    "\n",
    "def placeholder_msgs(msgs: t.List[t.Dict[str, str]]):\n",
    "    return [{\"role\": m[\"role\"], \"content\": GLYPH} for m in msgs]\n",
    "\n",
    "\n",
    "input_ids = granite.apply_chat_template(msgs)\n",
    "\n",
    "individual_msgs = [granite.encode(msg[\"content\"]) for msg in msgs]\n",
    "placeholder_ids = granite.apply_chat_template(placeholder_msgs(msgs))\n",
    "\n",
    "\n",
    "# print(individual_msgs)\n",
    "# print(input_ids)\n",
    "# print(placeholder_ids)\n",
    "\n",
    "i = 0\n",
    "glyph_id = granite.encode(GLYPH)\n",
    "print(glyph_id)\n",
    "\n",
    "# just try something shitty first\n",
    "ranges = []\n",
    "\n",
    "# \n",
    "\n",
    "i = 0\n",
    "while i < len(input_ids):\n",
    "    # look to start substring matching\n",
    "    if placeholder_ids[i] == glyph_id[0]:\n",
    "        print(f'potentially found a glyph id match starting at {i=}')\n",
    "        j = i\n",
    "        k = 0\n",
    "        matching = True\n",
    "        while k < len(glyph_id) and j < len(placeholder_ids):\n",
    "            # keep looking to see how far we can match against the glyphd ID\n",
    "            if placeholder_ids[j] != glyph_id[k]: \n",
    "                print(f'but unfortunately, found that at {k=}, {j=}, {placeholder_ids[j]=} != {glyph_id[k]=}')\n",
    "                matching = False\n",
    "                break\n",
    "\n",
    "            j += 1\n",
    "            k += 1\n",
    "        \n",
    "        # we were able to loop through successfully\n",
    "        if k == len(glyph_id) and matching:\n",
    "            # we now know that between `starti` and `i` there exists a range which is part of a tokenizer\n",
    "            ranges.append((i, j))\n",
    "            # print(f\"{k=}, {j=}, {placeholder_ids[j]=}\")\n",
    "\n",
    "            # now we can set `i` <-- j, and set `starti` <-- j + 1\n",
    "            i = j\n",
    "    i += 1\n",
    "\n",
    "assert len(ranges) == len(msgs)\n",
    "\n",
    "\n",
    "# next, lets collect the ranges and decode them to see what we get \n",
    "pieces = [granite.decode(placeholder_ids[i:j]) for i, j in ranges]\n",
    "# print(pieces)\n",
    "# print('=---')\n",
    "# print(granite.decode(input_ids))\n",
    "\n",
    "\n",
    "# basically the algorithm will look like this:\n",
    "# 1. given some list of messages, create a template set of messages with the contents replaced with a glyph\n",
    "# 2. tokenize the glyph messages and identify the portions in the message where the glyph exists\n",
    "# 3. with the tokenized list, identify the ranges where the glyph exists. We will want to replace these ranges with tokenized copies of each message\n",
    "# 4. with the knowledge of where the new message ranges are, we can now unmask according to our policy\n",
    "#   1. create a copy of the input IDs and leave the portions masked (-100) except for where we expect them to be unmasked\n",
    "#   2. when unmasking a particular message, if the tokenizer has an EOS token, assert that it is last token \n",
    "\n",
    "\n",
    "\n",
    "# the algorithm\n",
    "final_input_ids = []\n",
    "final_labels = []\n",
    "\n",
    "j = 0\n",
    "\n",
    "\n",
    "while j < len(placeholder_ids):\n",
    "    # remove one range\n",
    "    if not ranges:\n",
    "        # just append everything else to the end\n",
    "        final_input_ids.extend(placeholder_ids[j:])\n",
    "        final_labels.extend([-100] * len(placeholder_ids[j:]))\n",
    "        break\n",
    "    \n",
    "    start_idx, end_idx = ranges[0]\n",
    "    if j < start_idx:\n",
    "        # default case, just continue adding into input IDs and labels without doing anything\n",
    "        final_input_ids.append(placeholder_ids[j])\n",
    "        final_labels.append(-100)   # mask this out, we dont care about it\n",
    "        j += 1\n",
    "        continue\n",
    "    else:\n",
    "        # otherwise, we now must insert the tokenized user message. We select it via:\n",
    "        msg_idx = len(individual_msgs) - len(ranges)  # this should always select the correct message\n",
    "        msg = individual_msgs[msg_idx]\n",
    "\n",
    "        # now we can append the correct message into the input IDs with the proper masking\n",
    "        final_input_ids.extend(msg)\n",
    "        final_labels.extend(msg)\n",
    "\n",
    "        # continue only looking at the next set of ranges\n",
    "        j = end_idx\n",
    "        ranges = ranges[1:]\n",
    "\n",
    "        # we want to also unmask the EOS token if it is present\n",
    "        print(f\"after extending, {j=} is set to {placeholder_ids[j]=}\")\n",
    "\n",
    "        if granite.eos_token_id is not None:\n",
    "            print('detected eos token id, proceeding forward')\n",
    "            suffix_start_j = j\n",
    "            while j < len(placeholder_ids) and placeholder_ids[j] != granite.eos_token_id:\n",
    "                j += 1\n",
    "            \n",
    "            if j >= len(placeholder_ids) or placeholder_ids[j] != granite.eos_token_id:\n",
    "                raise RuntimeError('failed to find the trailing EOS token id')\n",
    "            \n",
    "            # by now we know that we are both within range and have found the trailing eos token id\n",
    "            final_input_ids.extend(placeholder_ids[suffix_start_j:j+1])\n",
    "            final_labels.extend(placeholder_ids[suffix_start_j:j+1])\n",
    "            j += 1\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "print(final_input_ids)\n",
    "print(final_labels)\n",
    "\n",
    "print(granite.decode(final_input_ids))\n",
    "assert granite.decode(final_input_ids) == granite.decode(input_ids)\n",
    "assert len(final_labels) == len(final_input_ids)\n",
    "\n",
    "print(granite.encode(granite.eos_token))\n",
    "\n",
    "_final_input_ids = final_input_ids[:]\n",
    "_final_labels = final_labels[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "granite.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [1, 2, 3]\n",
    "test[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189, 246, 227, 131]\n",
      "potentially found a glyph id match starting at i=3\n",
      "potentially found a glyph id match starting at i=12\n",
      "potentially found a glyph id match starting at i=21\n",
      "encoded glyph: [128000, 172, 241, 222, 112]\n",
      "potentially found a glyph id match starting at i=25\n",
      "potentially found a glyph id match starting at i=34\n",
      "potentially found a glyph id match starting at i=43\n",
      "after extending, j=29 is set to placeholder_ids[j]=128009\n",
      "detected eos token id, proceeding forward\n",
      "after extending, j=38 is set to placeholder_ids[j]=128009\n",
      "detected eos token id, proceeding forward\n",
      "after extending, j=47 is set to placeholder_ids[j]=128009\n",
      "detected eos token id, proceeding forward\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 1917, 0, 128009, -100, -100, -100, -100, 39, 50099, 17619, 128009, -100, -100, -100, -100, 9906, 505, 279, 1023, 3185, 128009]\n",
      "Hello world!<|eot_id|>Holey shit<|eot_id|>Hello from the other side<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "GLYPH = \"ð“€´\"\n",
    "\n",
    "def placeholder_msgs(msgs: t.List[t.Dict[str, str]]):\n",
    "    return [{\"role\": m[\"role\"], \"content\": GLYPH} for m in msgs]\n",
    "\n",
    "\n",
    "input_ids = granite.apply_chat_template(msgs)\n",
    "\n",
    "individual_msgs = [granite.encode(msg[\"content\"]) for msg in msgs]\n",
    "placeholder_ids = granite.apply_chat_template(placeholder_msgs(msgs))\n",
    "\n",
    "\n",
    "# print(individual_msgs)\n",
    "# print(input_ids)\n",
    "# print(placeholder_ids)\n",
    "\n",
    "i = 0\n",
    "glyph_id = granite.encode(GLYPH)\n",
    "print(glyph_id)\n",
    "\n",
    "# just try something shitty first\n",
    "ranges = []\n",
    "\n",
    "# \n",
    "\n",
    "i = 0\n",
    "while i < len(input_ids):\n",
    "    # look to start substring matching\n",
    "    if placeholder_ids[i] == glyph_id[0]:\n",
    "        print(f'potentially found a glyph id match starting at {i=}')\n",
    "        j = i\n",
    "        k = 0\n",
    "        matching = True\n",
    "        while k < len(glyph_id) and j < len(placeholder_ids):\n",
    "            # keep looking to see how far we can match against the glyphd ID\n",
    "            if placeholder_ids[j] != glyph_id[k]: \n",
    "                print(f'but unfortunately, found that at {k=}, {j=}, {placeholder_ids[j]=} != {glyph_id[k]=}')\n",
    "                matching = False\n",
    "                break\n",
    "\n",
    "            j += 1\n",
    "            k += 1\n",
    "        \n",
    "        # we were able to loop through successfully\n",
    "        if k == len(glyph_id) and matching:\n",
    "            # we now know that between `starti` and `i` there exists a range which is part of a tokenizer\n",
    "            ranges.append((i, j))\n",
    "            # print(f\"{k=}, {j=}, {placeholder_ids[j]=}\")\n",
    "\n",
    "            # now we can set `i` <-- j, and set `starti` <-- j + 1\n",
    "            i = j\n",
    "    i += 1\n",
    "\n",
    "assert len(ranges) == len(msgs)\n",
    "\n",
    "\n",
    "# next, lets collect the ranges and decode them to see what we get \n",
    "# print(pieces)\n",
    "# print('=---')\n",
    "# print(granite.decode(input_ids))\n",
    "\n",
    "\n",
    "# basically the algorithm will look like this:\n",
    "# 1. given some list of messages, create a template set of messages with the contents replaced with a glyph\n",
    "# 2. tokenize the glyph messages and identify the portions in the message where the glyph exists\n",
    "# 3. with the tokenized list, identify the ranges where the glyph exists. We will want to replace these ranges with tokenized copies of each message\n",
    "# 4. with the knowledge of where the new message ranges are, we can now unmask according to our policy\n",
    "#   1. create a copy of the input IDs and leave the portions masked (-100) except for where we expect them to be unmasked\n",
    "#   2. when unmasking a particular message, if the tokenizer has an EOS token, assert that it is last token \n",
    "\n",
    "\n",
    "def get_placeholder_ranges(placeholder_ids: t.List[int], tokenizer: PreTrainedTokenizer):\n",
    "    glyph_id = tokenizer.encode(GLYPH, add_special_tokens=False)  # we want to ignore special tokens since we're just extracting the token IDs here\n",
    "    ranges = []\n",
    "    i = 0\n",
    "    while i < len(placeholder_ids):\n",
    "        # look to start substring matching\n",
    "        if placeholder_ids[i] == glyph_id[0]:\n",
    "            print(f'potentially found a glyph id match starting at {i=}')\n",
    "            j = i\n",
    "            k = 0\n",
    "            matching = True\n",
    "            while k < len(glyph_id) and j < len(placeholder_ids):\n",
    "                # keep looking to see how far we can match against the glyphd ID\n",
    "                if placeholder_ids[j] != glyph_id[k]: \n",
    "                    print(f'but unfortunately, found that at {k=}, {j=}, {placeholder_ids[j]=} != {glyph_id[k]=}')\n",
    "                    matching = False\n",
    "                    break\n",
    "\n",
    "                j += 1\n",
    "                k += 1\n",
    "\n",
    "            # we were able to loop through successfully\n",
    "            if k == len(glyph_id) and matching:\n",
    "                # we now know that between `starti` and `i` there exists a range which is part of a tokenizer\n",
    "                ranges.append((i, j))\n",
    "\n",
    "                # now we can set `i` <-- j, and set `starti` <-- j + 1\n",
    "                i = j\n",
    "        i += 1\n",
    "\n",
    "    return ranges\n",
    "\n",
    "\n",
    "\n",
    "def unmask_messages(msgs: t.List[t.Dict[str, str]], tokenizer: PreTrainedTokenizer) -> t.Dict[str, t.List[int]]:\n",
    "    \"\"\"\n",
    "    Given a list of messages and an arbitrary tokenizer, returns a dictionary with\n",
    "    `input_ids` and `labels` containing the correct masking.\n",
    "    \"\"\"\n",
    "\n",
    "    # first we need to create the placeholder IDs\n",
    "    placeholder_ids = tokenizer.apply_chat_template(placeholder_msgs(msgs))\n",
    "    ranges = get_placeholder_ranges(placeholder_ids, tokenizer)\n",
    "    individual_msgs = [tokenizer.encode(m[\"content\"], add_special_tokens=False) for m in msgs]  # no special tokens here since we are looking to inject these into a broader template\n",
    "\n",
    "    final_input_ids = []\n",
    "    final_labels = []\n",
    "\n",
    "    j = 0\n",
    "    while j < len(placeholder_ids):\n",
    "        # remove one range\n",
    "        if not ranges:\n",
    "            # just append everything else to the end\n",
    "            final_input_ids.extend(placeholder_ids[j:])\n",
    "            final_labels.extend([-100] * len(placeholder_ids[j:]))\n",
    "            break\n",
    "        \n",
    "        start_idx, end_idx = ranges[0]\n",
    "        if j < start_idx:\n",
    "            # default case, just continue adding into input IDs and labels without doing anything\n",
    "            final_input_ids.append(placeholder_ids[j])\n",
    "            final_labels.append(-100)   # mask this out, we dont care about it\n",
    "            j += 1\n",
    "            continue\n",
    "        else:\n",
    "            # otherwise, we now must insert the tokenized user message. We select it via:\n",
    "            msg_idx = len(individual_msgs) - len(ranges)  # this should always select the correct message\n",
    "            msg = individual_msgs[msg_idx]\n",
    "\n",
    "            # now we can append the correct message into the input IDs with the proper masking\n",
    "            final_input_ids.extend(msg)\n",
    "            final_labels.extend(msg)\n",
    "\n",
    "            # continue only looking at the next set of ranges\n",
    "            j = end_idx\n",
    "            ranges = ranges[1:]\n",
    "\n",
    "            # we want to also unmask the EOS token if it is present\n",
    "            print(f\"after extending, {j=} is set to {placeholder_ids[j]=}\")\n",
    "\n",
    "            if granite.eos_token_id is not None:\n",
    "                print('detected eos token id, proceeding forward')\n",
    "                suffix_start_j = j\n",
    "                while j < len(placeholder_ids) and placeholder_ids[j] != tokenizer.eos_token_id:\n",
    "                    j += 1\n",
    "\n",
    "                if j >= len(placeholder_ids) or placeholder_ids[j] != tokenizer.eos_token_id:\n",
    "                    raise RuntimeError('failed to find the trailing EOS token id')\n",
    "\n",
    "                # by now we know that we are both within range and have found the trailing eos token id\n",
    "                final_input_ids.extend(placeholder_ids[suffix_start_j:j+1])\n",
    "                final_labels.extend(placeholder_ids[suffix_start_j:j+1])\n",
    "                j += 1\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": final_input_ids,\n",
    "        \"labels\": final_labels\n",
    "    }\n",
    "\n",
    "msgs = [\n",
    "    {\n",
    "        \"content\": \"Hello world!\",\n",
    "        \"role\": \"system\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Holey shit\",\n",
    "        \"role\": \"user\"\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Hello from the other side\",\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "]\n",
    "\n",
    "GLYPH = \"ð“€´\"\n",
    "print(f\"encoded glyph: {llama.encode(GLYPH)}\")\n",
    "\n",
    "results = unmask_messages(msgs, llama)\n",
    "results\n",
    "\n",
    "# assert results[\"input_ids\"] == _final_input_ids\n",
    "# assert results[\"labels\"] == _final_labels\n",
    "print(results[\"labels\"])\n",
    "# print(results[\"input_ids\"])\n",
    "\n",
    "# print(llama.apply_chat_template(placeholder_msgs(msgs)))\n",
    "\n",
    "\n",
    "# lets print out all unmasked tokens\n",
    "unmasked = [tok for tok in results[\"labels\"] if tok != -100]\n",
    "print(llama.decode(unmasked))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000 --> '<|begin_of_text|>'\n",
      "128006 --> '<|start_header_id|>'\n",
      "9125 --> 'system'\n",
      "128007 --> '<|end_header_id|>'\n",
      "271 --> '\\n\\n'\n",
      "38766 --> 'Cut'\n",
      "1303 --> 'ting'\n",
      "33025 --> ' Knowledge'\n",
      "2696 --> ' Date'\n",
      "25 --> ':'\n",
      "6790 --> ' December'\n",
      "220 --> ' '\n",
      "2366 --> '202'\n",
      "18 --> '3'\n",
      "198 --> '\\n'\n",
      "15724 --> 'Today'\n",
      "2696 --> ' Date'\n",
      "25 --> ':'\n",
      "220 --> ' '\n",
      "1627 --> '26'\n",
      "10263 --> ' Jul'\n",
      "220 --> ' '\n",
      "2366 --> '202'\n",
      "19 --> '4'\n",
      "271 --> '\\n\\n'\n",
      "172 --> 'ï¿½'\n",
      "241 --> 'ï¿½'\n",
      "222 --> 'ï¿½'\n",
      "112 --> 'ï¿½'\n",
      "128009 --> '<|eot_id|>'\n",
      "128006 --> '<|start_header_id|>'\n",
      "882 --> 'user'\n",
      "128007 --> '<|end_header_id|>'\n",
      "271 --> '\\n\\n'\n",
      "172 --> 'ï¿½'\n",
      "241 --> 'ï¿½'\n",
      "222 --> 'ï¿½'\n",
      "112 --> 'ï¿½'\n",
      "128009 --> '<|eot_id|>'\n",
      "128006 --> '<|start_header_id|>'\n",
      "78191 --> 'assistant'\n",
      "128007 --> '<|end_header_id|>'\n",
      "271 --> '\\n\\n'\n",
      "172 --> 'ï¿½'\n",
      "241 --> 'ï¿½'\n",
      "222 --> 'ï¿½'\n",
      "112 --> 'ï¿½'\n",
      "128009 --> '<|eot_id|>'\n"
     ]
    }
   ],
   "source": [
    "for tok in llama.apply_chat_template(placeholder_msgs(msgs)):\n",
    "    print(f'{tok} --> {repr(llama.decode(tok))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ð“€´', [172, 241, 222, 112])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.decode([172, 241, 222, 112]), llama.encode(GLYPH, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 13347, 11, 358, 2846, 7043]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
